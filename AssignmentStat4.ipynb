{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9304fd24-5468-41bd-989d-45b489564968",
   "metadata": {},
   "source": [
    "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n",
    "an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2eb37f-2396-4836-a14a-53cf51d22eaf",
   "metadata": {},
   "source": [
    "Probability Mass Function (PMF) and Probability Density Function (PDF) are two important concepts in probability theory and statistics.\n",
    "\n",
    "The PMF is a function that gives the probability of each possible value in a discrete random variable. It is defined as:\n",
    "\n",
    "PMF(x) = P(X = x)\n",
    "\n",
    "where X is a discrete random variable and x is a possible value of X.\n",
    "\n",
    "For example, consider a six-sided fair dice roll. The possible values of X are 1, 2, 3, 4, 5, and 6. The PMF of X is given by:\n",
    "\n",
    "PMF(1) = 1/6\n",
    "PMF(2) = 1/6\n",
    "PMF(3) = 1/6\n",
    "PMF(4) = 1/6\n",
    "PMF(5) = 1/6\n",
    "PMF(6) = 1/6\n",
    "\n",
    "The PMF of X shows that the probability of each possible value is 1/6, which is the same for all the values.\n",
    "\n",
    "The PDF, on the other hand, is a function that gives the probability density of each possible value in a continuous random variable. It is defined as:\n",
    "\n",
    "PDF(x) = dF(x)/dx\n",
    "\n",
    "where F is the cumulative distribution function (CDF) of a continuous random variable X.\n",
    "\n",
    "For example, consider the normal distribution with mean μ and standard deviation σ. The PDF of X is given by:\n",
    "\n",
    "PDF(x) = (1/σ√(2π)) * e^(-(x-μ)^2/(2σ^2))\n",
    "\n",
    "The PDF of X shows that the probability density of each possible value is determined by the shape of the normal distribution, which is characterized by its mean and standard deviation.\n",
    "\n",
    "In summary, the PMF is used for discrete random variables, while the PDF is used for continuous random variables. Both functions provide a way to describe the probability distribution of a random variable and are essential tools in statistical inference and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4bc90-dbea-4371-aef4-bc7cadb02518",
   "metadata": {},
   "source": [
    "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1f014-839f-443c-82d2-646406186362",
   "metadata": {},
   "source": [
    "Cumulative Density Function (CDF) is a function that gives the probability that a continuous random variable X is less than or equal to a specific value x, or the probability that a discrete random variable X is less than or equal to a specific value x. It is defined as:\n",
    "\n",
    "For a continuous random variable:\n",
    "CDF(x) = P(X ≤ x) = ∫f(t)dt from -∞ to x\n",
    "\n",
    "For a discrete random variable:\n",
    "CDF(x) = P(X ≤ x) = ΣP(X = i) for i ≤ x\n",
    "\n",
    "where f is the probability density function (PDF) for a continuous random variable, and P is the probability mass function (PMF) for a discrete random variable.\n",
    "\n",
    "For example, consider a continuous random variable X that follows a standard normal distribution. The CDF of X is given by:\n",
    "\n",
    "CDF(x) = P(X ≤ x) = ∫(1/√(2π)) * e^(-t^2/2)dt from -∞ to x\n",
    "\n",
    "The CDF of X shows the probability that X takes on a value less than or equal to a specific value x. For example, CDF(0) gives the probability that X is less than or equal to 0, which is about 0.5.\n",
    "\n",
    "The CDF is used in probability theory and statistics for several reasons. One of the main reasons is to determine the probability of a random variable taking on a specific range of values. For example, the probability that X lies between two values a and b can be calculated by taking the difference between the CDF values at b and a, that is, P(a < X ≤ b) = CDF(b) - CDF(a).\n",
    "\n",
    "Another reason for using CDF is to check if a given sample follows a specific probability distribution. If the empirical CDF of a sample is close to the theoretical CDF of a probability distribution, it indicates that the sample may come from that distribution.\n",
    "\n",
    "In summary, CDF is a function that gives the probability that a random variable is less than or equal to a specific value. CDF is used in probability theory and statistics to calculate probabilities and to test the fit of a sample to a probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c27bd2-8e77-4e7c-b380-dfbaa2d6d340",
   "metadata": {},
   "source": [
    "Q3: What are some examples of situations where the normal distribution might be used as a model?\n",
    "Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad2a38-9fc7-45ff-bebc-0e4ff81a26bb",
   "metadata": {},
   "source": [
    "The normal distribution is a commonly used probability distribution in statistics and it can be used as a model for a wide range of situations. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "1. Heights: The heights of adult humans tend to follow a normal distribution. The mean and standard deviation of the distribution can vary depending on the population being considered, but in general, the distribution is centered around a mean height and becomes less common as heights deviate from the mean.\n",
    "\n",
    "2. IQ scores: IQ scores are often modeled using a normal distribution, with a mean of 100 and a standard deviation of 15. This allows for easy comparison of individual scores to the population as a whole.\n",
    "\n",
    "3. Errors in measurement: The errors in many types of measurements, such as those made by a scale or a ruler, also tend to follow a normal distribution.\n",
    "\n",
    "4. Stock prices: The daily returns of many stocks also tend to follow a normal distribution. This has important implications for risk management and investment strategies.\n",
    "\n",
    "The parameters of the normal distribution are the mean (μ) and the standard deviation (σ). The mean determines the center of the distribution, while the standard deviation determines the spread of the distribution. Specifically:\n",
    "\n",
    "1. Increasing the mean shifts the distribution to the right, while decreasing the mean shifts it to the left.\n",
    "\n",
    "2. Increasing the standard deviation makes the distribution wider and flatter, while decreasing the standard deviation makes it narrower and taller.\n",
    "\n",
    "3. The shape of the normal distribution is symmetrical around the mean, meaning that the probabilities of values above and below the mean are equal.\n",
    "\n",
    "4. The empirical rule, also known as the 68-95-99.7 rule, states that approximately 68% of data falls within one standard deviation of the mean, about 95% falls within two standard deviations, and about 99.7% falls within three standard deviations. This rule also shows the relationship between the spread of the normal distribution and the probability of observing certain values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67344c-9a9b-49b6-bc67-101ed189f680",
   "metadata": {},
   "source": [
    "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n",
    "Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0c523-18d5-4450-8211-1d7497692f3f",
   "metadata": {},
   "source": [
    "The normal distribution is one of the most important probability distributions in statistics and has several practical applications. Here are some of the main reasons for the importance of normal distribution:\n",
    "\n",
    "1. Central Limit Theorem: The normal distribution is a key component of the Central Limit Theorem, which states that the sum or average of a large number of independent and identically distributed random variables tends to follow a normal distribution, regardless of the underlying distribution of the individual variables. This theorem has important implications for statistical inference and hypothesis testing.\n",
    "\n",
    "2. Approximation: Many real-life phenomena can be approximated by a normal distribution. This allows for easy modeling and prediction of future outcomes.\n",
    "\n",
    "3. Standardization: The normal distribution is often used to standardize data, which involves converting raw data into a standard form with a mean of 0 and a standard deviation of 1. This allows for easy comparison of data from different sources and also helps in hypothesis testing.\n",
    "\n",
    "Some examples of real-life situations that can be modeled using a normal distribution include:\n",
    "\n",
    "1. Test scores: In standardized tests like the SAT and ACT, the scores are often normally distributed with a mean around 500-600 and a standard deviation of 100.\n",
    "\n",
    "2. Heights: The heights of adult humans, as mentioned earlier, tend to follow a normal distribution.\n",
    "\n",
    "3. Blood pressure: The systolic blood pressure readings of healthy adults aged 18-50 years tend to follow a normal distribution, with a mean of around 120 and a standard deviation of around 15.\n",
    "\n",
    "4. IQ scores: IQ scores are also often modeled using a normal distribution, with a mean of 100 and a standard deviation of 15.\n",
    "\n",
    "5. Stock prices: The daily returns of many stocks, as mentioned earlier, tend to follow a normal distribution. This has important implications for risk management and investment strategies.\n",
    "\n",
    "In summary, normal distribution is important due to its role in statistical inference, approximation, and standardization. Normal distribution can be used to model many real-life situations, including test scores, heights, blood pressure, IQ scores, and stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b4839-cdce-4e94-826b-7eb841a00685",
   "metadata": {},
   "source": [
    "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n",
    "Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7d733-0b0b-47fb-9c70-bc751bf274b0",
   "metadata": {},
   "source": [
    "Bernoulli distribution is a discrete probability distribution that describes the probability of observing a binary outcome (success or failure) in a single trial of an experiment with a fixed probability of success (p). It is named after the Swiss mathematician Jacob Bernoulli.\n",
    "\n",
    "The Bernoulli distribution has only one parameter, p, which represents the probability of success. The probability mass function (PMF) of a Bernoulli distribution is given by:\n",
    "\n",
    "P(X = 1) = p\n",
    "P(X = 0) = 1 - p\n",
    "\n",
    "where X is the random variable representing the outcome of the trial.\n",
    "\n",
    "For example, consider the experiment of flipping a fair coin. The Bernoulli distribution can be used to model the probability of getting heads (success) or tails (failure) in a single flip. If we define success as getting heads, then the probability of success is p = 0.5, and the PMF of the Bernoulli distribution is:\n",
    "\n",
    "P(X = 1) = 0.5\n",
    "P(X = 0) = 0.5\n",
    "\n",
    "The difference between Bernoulli distribution and Binomial distribution is that Bernoulli distribution describes the probability of observing a binary outcome in a single trial, while Binomial distribution describes the probability of observing a certain number of successes in a fixed number of independent trials, each with the same probability of success.\n",
    "\n",
    "Binomial distribution has two parameters: the number of trials (n) and the probability of success in each trial (p). The PMF of a Binomial distribution is given by:\n",
    "\n",
    "P(X = k) = (n choose k) * p^k * (1-p)^(n-k)\n",
    "\n",
    "where X is the random variable representing the number of successes in n trials.\n",
    "\n",
    "For example, consider the experiment of flipping a fair coin 10 times. The Binomial distribution can be used to model the probability of getting a certain number of heads (successes) in the 10 flips. If we define success as getting heads, then the probability of success is p = 0.5, and the PMF of the Binomial distribution is:\n",
    "\n",
    "P(X = k) = (10 choose k) * 0.5^k * 0.5^(10-k) for k = 0, 1, 2, ..., 10\n",
    "\n",
    "In summary, Bernoulli distribution is used for modeling the probability of a binary outcome in a single trial, while Binomial distribution is used for modeling the probability of a certain number of successes in a fixed number of independent trials with the same probability of success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45051f70-6b50-4f87-a9de-847241324635",
   "metadata": {},
   "source": [
    "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661d68c-068c-4ead-85cd-9c04dea00efe",
   "metadata": {},
   "source": [
    "We are given that the dataset has a mean of 50 and a standard deviation of 10, and we want to find the probability that a randomly selected observation will be greater than 60, assuming that the dataset is normally distributed.\n",
    "\n",
    "We can use the standard normal distribution to calculate this probability. We first need to standardize the value of 60 using the formula:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "where x is the value we want to standardize (60 in this case), μ is the mean (50), and σ is the standard deviation (10).\n",
    "\n",
    "Plugging in the values, we get:\n",
    "\n",
    "z = (60 - 50) / 10 = 1\n",
    "\n",
    "This means that 60 is 1 standard deviation above the mean.\n",
    "\n",
    "Next, we use a standard normal distribution table or a calculator to find the probability that a z-score is greater than 1. This probability is approximately 0.1587.\n",
    "\n",
    "Therefore, the probability that a randomly selected observation from the dataset will be greater than 60 is approximately 0.1587 or 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fdc37-faec-4811-b331-a060206c46b6",
   "metadata": {},
   "source": [
    "Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a6792-3707-4c5b-93db-b8f0200eb2cb",
   "metadata": {},
   "source": [
    "Uniform distribution is a continuous probability distribution that has a constant probability density function (PDF) between two endpoints a and b, and zero density outside this interval. This means that any value in the interval has the same probability of being selected.\n",
    "\n",
    "The probability density function of a uniform distribution is given by:\n",
    "\n",
    "f(x) = 1 / (b - a) for a ≤ x ≤ b\n",
    "f(x) = 0 otherwise\n",
    "\n",
    "For example, consider the experiment of rolling a fair six-sided dice. The possible outcomes are the integers 1, 2, 3, 4, 5, and 6. Each outcome has an equal probability of 1/6.\n",
    "\n",
    "Now, let's modify the experiment by rolling a dice that has been loaded in such a way that the probability of rolling a number between 2 and 5 (inclusive) is twice the probability of rolling a 1 or a 6. This can be modeled using a uniform distribution with a = 1 and b = 6.\n",
    "\n",
    "The probability density function of the uniform distribution is:\n",
    "\n",
    "f(x) = 1 / (6 - 1) = 1/5 for 1 ≤ x ≤ 6\n",
    "f(x) = 0 otherwise\n",
    "\n",
    "Now, we need to adjust the probabilities of each outcome to match the desired probabilities. Since the probability of rolling a number between 2 and 5 is twice the probability of rolling a 1 or a 6, we can assign a probability of 2/7 to the numbers 2, 3, 4, and 5, and a probability of 1/7 to the numbers 1 and 6.\n",
    "\n",
    "The probability density function of the modified experiment is:\n",
    "\n",
    "f(x) = 1/5 for 1 ≤ x ≤ 6\n",
    "f(1) = 1/35\n",
    "f(2) = 2/35\n",
    "f(3) = 2/35\n",
    "f(4) = 2/35\n",
    "f(5) = 2/35\n",
    "f(6) = 1/35\n",
    "\n",
    "This shows that all outcomes in the interval [1,6] have the same probability density, while the probabilities of the endpoints 1 and 6 are half the probabilities of the other outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088cfa9-31df-4e0b-9675-be6b28bc1894",
   "metadata": {},
   "source": [
    "Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc1f92-6f97-44ac-ac95-85fc7ca9938a",
   "metadata": {},
   "source": [
    "A z-score, also known as a standard score, is a measure of how many standard deviations a data point or observation is away from the mean of a population. It is calculated by subtracting the population mean from the data point and then dividing the result by the population standard deviation. In mathematical notation, the formula for calculating the z-score is:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "where z is the z-score, x is the data point, μ is the population mean, and σ is the population standard deviation.\n",
    "\n",
    "The importance of the z-score lies in its ability to standardize data and facilitate comparisons across different populations or samples. By converting raw data into a standard form, the z-score allows us to compare observations from different datasets that have different units of measurement or different scales. It also helps in identifying extreme values or outliers in a dataset.\n",
    "\n",
    "Some important applications of the z-score include:\n",
    "\n",
    "1. Hypothesis testing: In statistical hypothesis testing, the z-score is used to calculate the p-value, which is the probability of observing a test statistic as extreme as or more extreme than the one observed, assuming the null hypothesis is true.\n",
    "\n",
    "2. Quality control: The z-score is used in quality control to identify outliers or data points that are significantly different from the mean, which can indicate a defect or a problem in the manufacturing process.\n",
    "\n",
    "3. Financial analysis: The z-score is used in financial analysis to calculate the credit-worthiness of a company or an individual, by comparing their financial ratios or credit scores to the population mean and standard deviation.\n",
    "\n",
    "In summary, the z-score is a standardized measure of how many standard deviations an observation is from the population mean. It is important in statistical inference, quality control, and financial analysis, and allows for easy comparisons across different populations or samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319ffbb-af5a-4491-9983-c4a85afb45bb",
   "metadata": {},
   "source": [
    "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0bee5-d40c-4264-b194-81824f2d7300",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics that states that the sum or average of a large number of independent and identically distributed (i.i.d) random variables tends towards a normal distribution, regardless of the underlying distribution of the individual variables.\n",
    "\n",
    "In mathematical terms, if X1, X2, ..., Xn are i.i.d random variables with mean μ and standard deviation σ, then the sum or average of these variables:\n",
    "\n",
    "- Sum: Y = ΣXi\n",
    "- Average: Y = (X1 + X2 + ... + Xn) / n\n",
    "\n",
    "will be approximately normally distributed with mean μY = nμ and standard deviation σY = σ√n, as n approaches infinity.\n",
    "\n",
    "The significance of the Central Limit Theorem is that it allows us to make inferences about the population mean or proportion, even when the population distribution is unknown or non-normal. This is because the distribution of the sample mean or proportion tends to become normal as the sample size increases, regardless of the underlying distribution of the population.\n",
    "\n",
    "The Central Limit Theorem plays a fundamental role in many areas of statistical analysis and inference, including hypothesis testing, confidence intervals, and estimating population parameters. It also provides a basis for the use of statistical techniques such as t-tests and ANOVA, which rely on the assumption of normality.\n",
    "\n",
    "In summary, the Central Limit Theorem is a fundamental concept in probability theory and statistics that states that the sum or average of a large number of i.i.d random variables tends towards a normal distribution. The significance of the theorem is that it allows us to make inferences about population parameters, even when the population distribution is unknown or non-normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f4078-235c-44cf-8964-ba38d6d3f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77aeccf-60f8-47ff-8345-587a5a278e46",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics that states that the sum or average of a large number of i.i.d random variables tends towards a normal distribution, regardless of the underlying distribution of the individual variables. However, to apply the CLT, certain assumptions must be met. The assumptions of the Central Limit Theorem are:\n",
    "\n",
    "1. Independence: The random variables must be independent of each other. This means that the value of one variable should not affect the value of another variable.\n",
    "\n",
    "2. Sample size: The sample size should be sufficiently large. While there is no hard and fast rule for what constitutes a \"large\" sample, a good rule of thumb is that the sample size should be at least 30.\n",
    "\n",
    "3. Identically distributed: The random variables should be identically distributed. This means that they should have the same mean and the same variance.\n",
    "\n",
    "4. Finite variance: The random variables should have a finite variance. This means that the distribution should not have excessive skewness or kurtosis.\n",
    "\n",
    "It is important to note that violating any of these assumptions can lead to inaccurate or misleading results. In particular, violating the assumption of independence can lead to biased estimates of the mean or standard deviation. Similarly, violating the assumption of normality can lead to incorrect inferences or tests.\n",
    "\n",
    "In summary, the Central Limit Theorem is a powerful tool in statistical inference, but its assumptions must be met in order to apply it correctly. The assumptions of the Central Limit Theorem are independence, sample size, identical distribution, and finite variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
